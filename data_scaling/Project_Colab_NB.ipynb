{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55296ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Data Scaling with SDV (CTGAN / WGAN-GP)\n",
    "'''\n",
    "This notebook trains an SDV CTGAN (WGAN-GP architecture) model on a cleaned\n",
    "transaction-level dataset to generate synthetic data for analytical stress-testing.\n",
    "\n",
    "Execution Environment:\n",
    "- Google Colab (Free Tier)\n",
    "- GPU-enabled runtime (CUDA preferred)\n",
    "\n",
    "Prerequisites (run before anything else):\n",
    "1. Install required libraries:\n",
    "   `!pip install sdv`\n",
    "2. Mount Google Drive to access input data and save trained models.\n",
    "   `from google.colab import drive`\n",
    "   `drive.mount('/content/drive')`\n",
    "\n",
    "Notes:\n",
    "- This notebook is not a production ML pipeline.\n",
    "- The model is used strictly for synthetic data generation, not prediction.\n",
    "- Personal identifiers were intentionally excluded to preserve stability and focus on aggregate patterns.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be374714",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important\n",
    "'''\n",
    "Additional Usage Note:\n",
    "- File paths and filenames can be changed freely.\n",
    "  Ensure naming and path consistency is preserved across training,\n",
    "  generation, and downstream processing.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3679710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SDV (required for Colab)\n",
    "!pip install sdv\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a5e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "import gc\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. HARDWARE CHECK\n",
    "# ------------------------------------------------------------------------------\n",
    "# Prefer GPU execution when available (WGAN-GP benefits significantly from CUDA)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Hardware detected: training will run on {device.upper()}\")\n",
    "\n",
    "# Free memory before training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. LOAD DATA\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"Loading cleaned transaction data...\")\n",
    "df_trans = pd.read_csv(\n",
    "    '/content/drive/MyDrive/DataCo_Synthetic/fixed_columns_final.csv'\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. METADATA DEFINITION\n",
    "# ------------------------------------------------------------------------------\n",
    "# Automatically infer column types from the dataframe\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(df_trans)\n",
    "\n",
    "# Manual overrides to preserve analytical meaning and stability\n",
    "# - Prevent SDV from treating state fields as PII\n",
    "# - Enforce date components as categorical to avoid fractional values\n",
    "metadata.update_column(column_name='customer_state', sdtype='categorical')\n",
    "metadata.update_column(column_name='order_state', sdtype='categorical')\n",
    "\n",
    "metadata.update_column(column_name='order_year', sdtype='categorical')\n",
    "metadata.update_column(column_name='order_month', sdtype='categorical')\n",
    "metadata.update_column(column_name='order_day', sdtype='categorical')\n",
    "\n",
    "print(\"Final metadata configuration:\")\n",
    "print(metadata.to_dict())\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. CONFIGURE CTGAN (WGAN-GP)\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"Initializing CTGAN (WGAN-GP architecture)...\")\n",
    "\n",
    "synthesizer = CTGANSynthesizer(\n",
    "    metadata,\n",
    "    epochs=500,       # Higher epochs for improved convergence\n",
    "    batch_size=500,   # Smaller batches for more stable gradient updates\n",
    "    verbose=True,\n",
    "    cuda=True         # Force GPU usage when available\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. TRAIN MODEL\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"Starting CTGAN training...\")\n",
    "synthesizer.fit(df_trans)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. SAVE TRAINED MODEL\n",
    "# ------------------------------------------------------------------------------\n",
    "save_path = '/content/drive/MyDrive/DataCo_Synthetic/CTGAN_WGAN_ModelEpochs500.pkl'\n",
    "synthesizer.save(save_path)\n",
    "\n",
    "print(f\"Model saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d28956",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Synthetic Data Quality Evaluation (SDV)\n",
    "'''\n",
    "This section evaluates the quality of the generated synthetic dataset against\n",
    "the original data using SDV's built-in diagnostics and statistical similarity checks.\n",
    "\n",
    "Purpose:\n",
    "- Validate that synthetic data preserves structural and statistical properties\n",
    "- Ensure relationships are usable for analytical stress-testing\n",
    "- Confirm no major distributional or constraint violations\n",
    "\n",
    "Notes:\n",
    "- Evaluation is performed on samples for efficiency\n",
    "- Scores are used as sanity checks, not as ML benchmarks\n",
    "- High scores indicate analytical usability, not predictive accuracy\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.evaluation.single_table import evaluate_quality, run_diagnostic\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. LOAD REAL AND SYNTHETIC DATA (SAMPLED)\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"Loading data for quality evaluation...\")\n",
    "\n",
    "# Real data sample (sufficient for statistical comparison)\n",
    "real_df = pd.read_csv(\n",
    "    '/content/drive/MyDrive/DataCo_Synthetic/fixed_columns_final.csv',\n",
    "    nrows=150_000\n",
    ")\n",
    "\n",
    "# Synthetic data sample generated by CTGAN\n",
    "fake_df = pd.read_csv(\n",
    "    '/content/drive/MyDrive/DataCo_Synthetic/DataCo_Synthetic_2M_Safe.csv',\n",
    "    nrows=1_500_000\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. METADATA SETUP\n",
    "# ------------------------------------------------------------------------------\n",
    "# Metadata is inferred from real data to define valid structure and constraints\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(real_df)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. DIAGNOSTIC CHECK\n",
    "# ------------------------------------------------------------------------------\n",
    "# Verifies whether the synthetic data violates basic rules or constraints\n",
    "print(\"\\nRunning diagnostic checks...\")\n",
    "diagnostic = run_diagnostic(\n",
    "    real_data=real_df,\n",
    "    synthetic_data=fake_df,\n",
    "    metadata=metadata\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. STATISTICAL QUALITY EVALUATION\n",
    "# ------------------------------------------------------------------------------\n",
    "# Measures how closely synthetic distributions match the real data\n",
    "print(\"\\nRunning statistical quality evaluation...\")\n",
    "report = evaluate_quality(\n",
    "    real_data=real_df,\n",
    "    synthetic_data=fake_df,\n",
    "    metadata=metadata\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. RESULTS SUMMARY\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(f\"Synthetic Data Quality Score: {report.get_score() * 100:.2f}%\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\nDetailed Metric Breakdown:\")\n",
    "print(report.get_properties())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b8a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quick Synthetic Sample Validation\n",
    "'''\n",
    "This section performs a lightweight quality check on a small synthetic sample\n",
    "generated directly from the trained CTGAN model.\n",
    "\n",
    "Purpose:\n",
    "- Sanity-check the trained model before large-scale generation\n",
    "- Verify that basic statistical structure is preserved\n",
    "- Catch obvious metadata or training issues early\n",
    "\n",
    "Notes:\n",
    "- This is a fast validation step, not a full benchmark\n",
    "- Results are indicative, not final\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeacd202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# QUICK SYNTHETIC SAMPLE QUALITY CHECK\n",
    "# ------------------------------------------------------------------------------\n",
    "from sdv.evaluation.single_table import evaluate_quality, run_diagnostic\n",
    "\n",
    "# Generate a synthetic sample directly from the trained model\n",
    "print(\"Generating synthetic sample for validation...\")\n",
    "sample_500 = synthesizer.sample(num_rows=200_000)\n",
    "\n",
    "# Run statistical quality evaluation\n",
    "print(\"Running quality evaluation on synthetic sample...\")\n",
    "quality_report = evaluate_quality(\n",
    "    real_data=df_trans,\n",
    "    synthetic_data=sample_500,\n",
    "    metadata=metadata\n",
    ")\n",
    "\n",
    "# Output overall quality score\n",
    "print(\"\\nFinal synthetic data quality score:\")\n",
    "print(quality_report.get_score())\n",
    "\n",
    "# Optional diagnostic check (useful for debugging schema or constraint issues)\n",
    "# diagnostic_report = run_diagnostic(\n",
    "#     real_data=df_trans,\n",
    "#     synthetic_data=sample_500,\n",
    "#     metadata=metadata\n",
    "# )\n",
    "# print(diagnostic_report.get_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Large-Scale Synthetic Data Generation with Safety Constraints\n",
    "'''\n",
    "This section generates a large synthetic dataset (~2M rows) from the trained\n",
    "CTGAN model while enforcing real-world relational constraints.\n",
    "\n",
    "Purpose:\n",
    "- Scale the dataset for analytical stress-testing\n",
    "- Prevent unrealistic or hallucinated combinations\n",
    "- Preserve valid geography, product hierarchy, and customer mappings\n",
    "\n",
    "Approach:\n",
    "- Generate data in chunks to manage memory\n",
    "- Reapply \"truth tables\" derived from real data\n",
    "- Enforce valid combinations post-generation\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a852af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "import gc\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------------------------------------------------------------\n",
    "MODEL_PATH = '/content/drive/MyDrive/DataCo_Synthetic/DataCo_WGAN_Model_Epochs500.pkl'\n",
    "ORIGINAL_DATA_PATH = '/content/drive/MyDrive/DataCo_Synthetic/fixed_columns_final.csv'\n",
    "OUTPUT_FILENAME = '/content/drive/MyDrive/DataCo_Synthetic/DataCo_Final_2M.csv'\n",
    "\n",
    "NUM_ROWS_TO_GENERATE = 2_000_000\n",
    "CHUNK_SIZE = 100_000\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. LOAD MODEL AND REFERENCE DATA\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"Loading trained CTGAN model and reference dataset...\")\n",
    "\n",
    "synthesizer = CTGANSynthesizer.load(MODEL_PATH)\n",
    "df_real = pd.read_csv(ORIGINAL_DATA_PATH)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. BUILD REFERENCE CONSTRAINT TABLES (\"TRUTH TABLES\")\n",
    "# ------------------------------------------------------------------------------\n",
    "# These tables are used to re-enforce valid combinations after sampling\n",
    "\n",
    "print(\"Building reference constraint tables...\")\n",
    "\n",
    "# Order geography constraints\n",
    "valid_order_geo = df_real[\n",
    "    [\"order_state\", \"order_country\", \"order_region\", \"market\"]\n",
    "].drop_duplicates()\n",
    "\n",
    "# Product hierarchy constraints\n",
    "valid_products = df_real[\n",
    "    [\"product_name\", \"category_name\", \"department_name\"]\n",
    "].drop_duplicates()\n",
    "\n",
    "# Customer geography constraints\n",
    "valid_customer_geo = df_real[\n",
    "    [\"customer_state\", \"customer_country\"]\n",
    "].drop_duplicates()\n",
    "\n",
    "print(f\"Valid order geography combinations: {len(valid_order_geo)}\")\n",
    "print(f\"Valid customer geography combinations: {len(valid_customer_geo)}\")\n",
    "print(f\"Valid product combinations: {len(valid_products)}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. SYNTHETIC DATA GENERATION (CHUNKED)\n",
    "# ------------------------------------------------------------------------------\n",
    "total_chunks = NUM_ROWS_TO_GENERATE // CHUNK_SIZE\n",
    "synthetic_batches = []\n",
    "\n",
    "print(f\"Starting synthetic generation of {NUM_ROWS_TO_GENERATE} rows...\")\n",
    "\n",
    "for i in range(total_chunks):\n",
    "    print(f\"Generating batch {i + 1}/{total_chunks}\")\n",
    "\n",
    "    batch = synthesizer.sample(num_rows=CHUNK_SIZE)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # APPLY SAFETY CONSTRAINTS\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Enforce valid order geography\n",
    "    batch = batch.drop(columns=[\"order_country\", \"order_region\", \"market\"])\n",
    "    batch = batch.merge(valid_order_geo, on=\"order_state\", how=\"inner\")\n",
    "\n",
    "    # Enforce valid product hierarchy\n",
    "    batch = batch.drop(columns=[\"category_name\", \"department_name\"])\n",
    "    batch = batch.merge(valid_products, on=\"product_name\", how=\"inner\")\n",
    "\n",
    "    # Enforce valid customer geography\n",
    "    batch = batch.drop(columns=[\"customer_country\"])\n",
    "    batch = batch.merge(valid_customer_geo, on=\"customer_state\", how=\"inner\")\n",
    "\n",
    "    synthetic_batches.append(batch)\n",
    "\n",
    "    # Explicit memory cleanup between batches\n",
    "    del batch\n",
    "    gc.collect()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. FINAL ASSEMBLY AND EXPORT\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"Combining generated batches...\")\n",
    "\n",
    "df_final = pd.concat(synthetic_batches, ignore_index=True)\n",
    "\n",
    "# Optional: downsample to exact row count for determinism\n",
    "df_final = df_final.sample(n=NUM_ROWS_TO_GENERATE, random_state=42)\n",
    "\n",
    "print(\"Synthetic data generation complete.\")\n",
    "print(f\"Final row count: {len(df_final)}\")\n",
    "\n",
    "print(f\"Saving output to: {OUTPUT_FILENAME}\")\n",
    "df_final.to_csv(OUTPUT_FILENAME, index=False)\n",
    "\n",
    "print(\"Generation process finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
