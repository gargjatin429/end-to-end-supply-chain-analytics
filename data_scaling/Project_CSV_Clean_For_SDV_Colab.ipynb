{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ⚠️ Important Note (Read Before Running Anything)\n",
    "'''\n",
    "This notebook exists **only** to clean and normalize the original Kaggle CSV\n",
    "so it can be safely used for **model training and synthetic data generation**.\n",
    "\n",
    "Why this matters:\n",
    "- SDV / CTGAN expects a **clean, stable CSV**\n",
    "- Broken encodings, bad dates, and messy column names will cause training to fail\n",
    "- This cleanup is mandatory before feeding data to any model\n",
    "\n",
    "Scope:\n",
    "- The output of this notebook is a **model-ready CSV**\n",
    "- It is not intended for analysis or dashboards\n",
    "- All blocks are intentionally separate and may be run independently\n",
    "\n",
    "If you skip this step and jump straight to training, the model will break.\n",
    "That's not a bug. That's on you.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e8e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initial Text Normalization & Column Pruning (Raw Kaggle Cleanup)\n",
    "'''\n",
    "This block performs aggressive text normalization on the original Kaggle\n",
    "DataCo Supply Chain dataset to eliminate encoding issues and unusable columns.\n",
    "\n",
    "Purpose:\n",
    "- Remove accented and corrupted Unicode characters\n",
    "- Normalize all text fields to plain ASCII\n",
    "- Drop unused, sensitive, or redundant columns\n",
    "- Produce a stable, model-safe CSV for downstream processing\n",
    "\n",
    "Notes:\n",
    "- This is a one-time cleanup step for a single raw file\n",
    "- Blocks in this notebook are intentionally independent\n",
    "- File paths and names can be adjusted as needed\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Normalize accented or special Unicode characters into plain ASCII.\n",
    "    Example: 'São Paulo' -> 'Sao Paulo'\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Decompose Unicode characters (e.g., é -> e + accent)\n",
    "    # Retain only base characters\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. LOAD RAW DATA\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"Loading raw dataset for text normalization...\")\n",
    "\n",
    "# 'latin-1' is more forgiving for this dataset's encoding issues\n",
    "df = pd.read_csv(\n",
    "    'DataCoSupplyChainDataset.csv',\n",
    "    encoding='latin-1'\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. NORMALIZE COLUMN NAMES\n",
    "# ------------------------------------------------------------------------------\n",
    "# Strip accidental whitespace from column headers\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. APPLY TEXT NORMALIZATION ACROSS ALL TEXT COLUMNS\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"Normalizing special characters across text columns...\")\n",
    "\n",
    "text_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in text_cols:\n",
    "    df[col] = df[col].apply(clean_text)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. DROP UNUSED / SENSITIVE / REDUNDANT COLUMNS\n",
    "# ------------------------------------------------------------------------------\n",
    "cols_to_drop = [\n",
    "    'Customer Email', 'Customer Fname', 'Customer Lname', 'Customer Password',\n",
    "    'Customer Street', 'Customer Zipcode', 'Order Id', 'Order Item Id',\n",
    "    'Customer Id', 'Order Customer Id', 'Product Card Id',\n",
    "    'Order Item Cardprod Id', 'Category Id', 'Department Id',\n",
    "    'Product Category Id', 'Product Description', 'Product Image',\n",
    "    'Latitude', 'Longitude', 'Benefit per order', 'Sales per customer',\n",
    "    'Delivery Status', 'Late_delivery_risk', 'Customer City', 'Order City',\n",
    "    'Order Item Discount', 'Sales', 'Order Item Total', 'Order Profit Per Order',\n",
    "    'Order Zipcode', 'Product Price', 'Product Status',\n",
    "    'shipping date (DateOrders)'\n",
    "]\n",
    "\n",
    "# Drop only columns that actually exist\n",
    "existing_cols = [c for c in cols_to_drop if c in df.columns]\n",
    "df_clean = df.drop(columns=existing_cols)\n",
    "\n",
    "print(f\"Dropped {len(existing_cols)} columns.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. EXPORT CLEANED DATA\n",
    "# ------------------------------------------------------------------------------\n",
    "# 'utf-8-sig' ensures Excel correctly interprets UTF-8 encoding\n",
    "output_name = 'DataCo_Cleaned_Plain_English.csv'\n",
    "df_clean.to_csv(\n",
    "    output_name,\n",
    "    index=False,\n",
    "    encoding='utf-8-sig'\n",
    ")\n",
    "\n",
    "print(f\"Cleanup complete. Output saved as: {output_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d734ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date Parsing & Temporal Repair (Raw Kaggle Fix)\n",
    "'''\n",
    "This block repairs corrupted and inconsistently formatted order date fields\n",
    "from the original Kaggle dataset.\n",
    "\n",
    "Purpose:\n",
    "- Parse multiple inconsistent datetime formats\n",
    "- Extract year, month, and day as atomic columns\n",
    "- Correct malformed years (e.g., 0018 → 2018)\n",
    "- Recompute day-of-week after date correction\n",
    "\n",
    "Notes:\n",
    "- This is a best-effort cleanup for a single raw file\n",
    "- Imperfect rows are tolerated and corrected logically\n",
    "- This step exists solely to stabilize downstream analysis\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1. FILE SETUP\n",
    "# ------------------------------------------------------------------------------\n",
    "folder_path = r\"D:\\Data Lake\\very_raw\"\n",
    "file_name = \"supply_chain_sample_new_2.csv\"\n",
    "full_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "df = pl.read_csv(full_path)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2. CLEAN & PARSE DATE STRINGS\n",
    "# ------------------------------------------------------------------------------\n",
    "# Normalize separators and spacing before parsing\n",
    "df = df.with_columns(\n",
    "    date_str_clean=(\n",
    "        pl.col(\"order date (DateOrders)\")\n",
    "        .str.replace_all(\"/\", \"-\")\n",
    "        .str.replace_all(\"  \", \" \")\n",
    "        .str.strip_chars()\n",
    "    )\n",
    ")\n",
    "\n",
    "# Attempt multiple datetime formats (best-effort parsing)\n",
    "df_final = df.with_columns(\n",
    "    parsed_date=pl.coalesce(\n",
    "        pl.col(\"date_str_clean\").str.to_datetime(\"%m-%d-%Y %H:%M\", strict=False),\n",
    "        pl.col(\"date_str_clean\").str.to_datetime(\"%m-%d-%Y %I:%M:%S %p\", strict=False),\n",
    "        pl.col(\"date_str_clean\").str.to_datetime(\"%m-%d-%y %H:%M\", strict=False)\n",
    "    )\n",
    ").with_columns(\n",
    "    Order_Year=pl.col(\"parsed_date\").dt.year(),\n",
    "    Order_Month=pl.col(\"parsed_date\").dt.month(),\n",
    "    Order_Day=pl.col(\"parsed_date\").dt.day()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3. TEMPORAL CORRECTION (YEAR FIX)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Fix malformed years (e.g., 18 -> 2018)\n",
    "df_final = df_final.with_columns(\n",
    "    Order_Year=pl.when(pl.col(\"Order_Year\") < 1900)\n",
    "                  .then(pl.col(\"Order_Year\") + 2000)\n",
    "                  .otherwise(pl.col(\"Order_Year\"))\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4. RECOMPUTE DAY OF WEEK\n",
    "# ------------------------------------------------------------------------------\n",
    "# Recalculate weekday after correcting the year\n",
    "df_final = df_final.with_columns(\n",
    "    Order_DayOfWeek=pl.date(\n",
    "        pl.col(\"Order_Year\"),\n",
    "        pl.col(\"Order_Month\"),\n",
    "        pl.col(\"Order_Day\")\n",
    "    ).dt.weekday()\n",
    ").drop([\"date_str_clean\", \"parsed_date\"])\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5. VALIDATION CHECK\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"Validating year correction...\")\n",
    "\n",
    "ancient_check = df_final.filter(pl.col(\"Order_Year\") < 1900)\n",
    "\n",
    "if ancient_check.height == 0:\n",
    "    print(\"All years successfully corrected.\")\n",
    "    print(\n",
    "        df_final.select(\n",
    "            [\"order date (DateOrders)\", \"Order_Year\", \"Order_DayOfWeek\"]\n",
    "        ).head(10)\n",
    "    )\n",
    "else:\n",
    "    print(f\"Found {ancient_check.height} rows with invalid years.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6. EXPORT CLEANED DATA\n",
    "# ------------------------------------------------------------------------------\n",
    "output_filename = \"Cleaned_Data_With_Dates.csv\"\n",
    "output_path = os.path.join(folder_path, output_filename)\n",
    "\n",
    "print(f\"Saving cleaned dataset to: {output_path}\")\n",
    "\n",
    "df_final.write_csv(\n",
    "    output_path,\n",
    "    separator=\",\"\n",
    ")\n",
    "\n",
    "print(\"Date cleanup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Column Name Standardization (Final Schema Hygiene)\n",
    "'''\n",
    "This block standardizes column names after all structural and date fixes\n",
    "have been applied.\n",
    "\n",
    "Purpose:\n",
    "- Enforce consistent snake_case naming\n",
    "- Remove spaces, brackets, and malformed characters\n",
    "- Prepare columns for downstream SQL, Polars, and BI compatibility\n",
    "\n",
    "Notes:\n",
    "- This step is intentionally isolated\n",
    "- Naming logic is explicit to avoid silent schema drift\n",
    "- Output is the final, model-ready dataset\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652e9081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# COLUMN NAME STANDARDIZATION\n",
    "# ------------------------------------------------------------------------------\n",
    "folder_path = r\"D:\\Data Lake\\very_raw\"\n",
    "file_name = \"Cleaned_Data_With_Dates.csv\"\n",
    "full_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "df2 = pl.read_csv(full_path)\n",
    "\n",
    "new_columns = []\n",
    "\n",
    "for col in df2.columns:\n",
    "    # Step 1: Strip leading/trailing whitespace\n",
    "    clean_name = col.strip()\n",
    "\n",
    "    # Step 2: Normalize casing\n",
    "    clean_name = clean_name.lower()\n",
    "\n",
    "    # Step 3: Replace problematic characters with underscores\n",
    "    clean_name = (\n",
    "        clean_name\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"(\", \"_\")\n",
    "        .replace(\")\", \"_\")\n",
    "        # Extend replacements here if additional symbols appear\n",
    "    )\n",
    "\n",
    "    # Step 4: Collapse multiple underscores\n",
    "    while \"__\" in clean_name:\n",
    "        clean_name = clean_name.replace(\"__\", \"_\")\n",
    "\n",
    "    # Step 5: Trim underscores from edges\n",
    "    clean_name = clean_name.strip(\"_\")\n",
    "\n",
    "    new_columns.append(clean_name)\n",
    "\n",
    "# Apply cleaned column names\n",
    "df2.columns = new_columns\n",
    "\n",
    "# Verify results\n",
    "print(\"Column name standardization complete.\")\n",
    "print(df2.columns)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# EXPORT FINAL CLEAN DATASET\n",
    "# ------------------------------------------------------------------------------\n",
    "output_filename = \"fixed_columns_final.csv\"\n",
    "output_path = os.path.join(folder_path, output_filename)\n",
    "\n",
    "df2.write_csv(output_path, separator=\",\")\n",
    "\n",
    "print(f\"Final dataset saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
